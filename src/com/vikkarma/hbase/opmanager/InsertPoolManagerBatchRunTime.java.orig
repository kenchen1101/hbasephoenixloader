package com.vikkarma.hbase.opmanager;

import java.util.Arrays;
import java.util.List;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.logging.Level;

import com.vikkarma.hbase.data.HBaseRecord;
import com.vikkarma.hbase.data.SplitRowKeyGenerator;
import com.vikkarma.hbase.loader.HBaseLoadParams;
import com.vikkarma.hbase.opthreads.GenerateBatchRunTimeThread;
import com.vikkarma.hbase.opthreads.InsertBatchRunTimeThread;
import com.vikkarma.utils.HBaseTestProperties;
import com.vikkarma.utils.MyLogger;

public class InsertPoolManagerBatchRunTime extends WriteLoadManager implements Runnable {
  // Insert test properties
  private static final int INSERT_BULK = HBaseLoadParams.getWriteBatchSize();

  // Concurrency Modeling
  private static final HBaseTestProperties testProperties = HBaseTestProperties.getInstance();
  private static final int PUTGEN_HBASEIN_QUEUE_SIZE = testProperties
      .getInteger("PUTGEN_HBASEIN_QUEUE_SIZE"); // numHConn
  private static ThreadPoolExecutor hbaseBatchGeneratorPool;
  // Queue for the hbaseBatchGeneratorPool and hbaseWriterPool to exchange batches to write on HBase
  BlockingQueue<List<HBaseRecord>> blockingQueue = null;

  // Manager latches
  CountDownLatch batchGeneratorLatch = null;
  CountDownLatch batchWriterLatch = null;

  public InsertPoolManagerBatchRunTime(CountDownLatch cdl) {
    super(cdl);
    // Generate put batches and insert into queue
    hbaseBatchGeneratorPool = (ThreadPoolExecutor) Executors.newFixedThreadPool(1);
    blockingQueue = new ArrayBlockingQueue<List<HBaseRecord>>(PUTGEN_HBASEIN_QUEUE_SIZE);
  }

  /**
   * Read the hbase test data file one bulk at a time and insert it into hbase using a pool of
   * InsertHBaseThread
   */

  @Override
  public void run() {
    // Start the put generator thread
    MyLogger.mylogger.info("Starting InsertPoolManagerBatchRunTime put batch generator thread ...");
    batchGeneratorLatch = new CountDownLatch(1);
    hbaseBatchGeneratorPool.execute(new GenerateBatchRunTimeThread(blockingQueue, false,
        batchGeneratorLatch));

    // Start the HBase writer threads
    MyLogger.mylogger.info("Starting InsertPoolManagerBatchRunTime ThreadPool Size["
        + HBaseLoadParams.getNumWriters() + "] to write " + numPutBatches
        + " batches to HBase of size " + HBaseLoadParams.getWriteBatchSize());
    batchWriterLatch = new CountDownLatch(numPutBatches);
    
    long beforetime = System.currentTimeMillis();
    int batchCtr = 0;
    int insertCounter = 0;
    // Generate data such that you have 1 bulk per ET from each region sequenced one after the other
    for (int keys = 0; keys < SplitRowKeyGenerator.TOTAL_KEYS; keys = insertCounter) {
      for (int dataInSplitIndex = 0; dataInSplitIndex < SplitRowKeyGenerator.NUM_ETS_PER_SPLIT; dataInSplitIndex++) {
        for (int splitRegion = 0; splitRegion < SplitRowKeyGenerator.NUM_REGION_SPLITS; splitRegion++) {
          if (batchCtr % 100 == 0) {
            MyLogger.mylogger.info("Picking up batch [" + batchCtr
                + "] from the queue to insert into HBase cluster");
          }
          // threadPool.execute(new InsertBatchRunTimeThread(blockingQueue, txnCtr,
          // batchWriterLatch));
          final Future<String> handler =
              scheduledthreadPool.submit(new InsertBatchRunTimeThread(blockingQueue, txnCtr,
                  batchWriterLatch));
          taskFutureList.add(handler);
          //Kill the insert thread if it takes more than 10 seconds
          //Observing issues related to some threads getting hung forever
          cancelthreadPool.schedule(new Runnable() {
            public void run() {
              handler.cancel(true);              
            }
          }, 10, TimeUnit.SECONDS);

          // Every batch inserts INSERT_BULK rowkeys
          batchCtr++;
          insertCounter = insertCounter + INSERT_BULK;
        }
      }
    }
    try {
      MyLogger.mylogger.info("Waiting for all batches[" + numPutBatches
        + "] in executor queue to complete...");
      batchWriterLatch.await();
      MyLogger.mylogger.info("Completed all batches[" + numPutBatches
        + "] in executor queue to complete...");
      // batchGeneratorLatch.await();
      // batchWriterLatch.await(7200, TimeUnit.SECONDS);
    } catch (InterruptedException e) {
      MyLogger.mylogger.log(Level.SEVERE, "Interrupted waiting for batches to complete",
        e.getMessage() + Arrays.toString(e.getStackTrace()));
    }
    
    //check taskFutureList and update 
    //batch failure count accordingly
    for (Future<String> fut : taskFutureList){
      if(fut.isCancelled()) {
        MyLogger.mylogger.severe("Detected a cancelled task taking more than 10 seconds in HBase insertion");
        //Count decremented in the insert thread itself
        //txnCtr.decrementCount();
      }
    }
    
    // Shutdown the batch generator pool
    hbaseBatchGeneratorPool.shutdown();
    cleanUp(beforetime);
  }
}
