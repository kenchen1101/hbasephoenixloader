#=============
#Environment
#Primary#DR (at present only 1 primary and 1 dr cluster is supported)
#=============
#add zk quorum here
MULTI_HBASE_HOST=localhost
HBASE_PORT=2181
IS_REPLICATION_ENABLED=false

#===========
#Table
#===========
TABLE_NAME=HT_P
SPLIT_TABLE=true
#If num splits are very less and num rows very large
#getRegionSplits might not work properly (need to fix)
NUM_REGION_SPLITS=4

#===========
#Data File
#===========
#disable WAL put for HBase throughput test 
WAL_PUT=true
#if true generates all the put batches in memory before starting HBase insertion
#Can be set to true only for small data sets to chack max possible tput
IN_MEM_BATCH=false
#Data file will not be generated. Data batches will be generated in runtime and 
#inserted in HBase to save file I/O time and increase client throughput
#if RUNTIME_BATCH_GEN=true then we have to set GENERATE_HISTORY_DATA=true
RUNTIME_BATCH_GEN=true
PUTGEN_HBASEIN_QUEUE_SIZE=40
GENERATE_HISTORY_DATA=true
# Generate fixed sized value 
GENERATE_FIXED_SIZE_DATA=true
# Max data length
MAX_DATA_LENGTH=100
#if GENERATE_KEYS_ONLY=true, data will be one character only 'a' , else it will be random length string
GENERATE_KEYS_ONLY=true
HISTORY_FILE_NAME=sgdata/hbase_test_data.csv
KEY_FILE_NAME=sgdata/hbase_split_keys.csv
#NUM_EXCHANGE*NUM_TOPICS*NUM_KEYS_PER_ET should be a multiple of NUM_REGION_SPLITS for uniform table split size
NUM_EXCHANGE=30
NUM_TOPICS=30
#NUM_KEYS_PER_ET should be greater than or equal to and a multiple of INSERT_BULK
NUM_KEYS_PER_ET=300
#for every addrecord add MULTIPLE_MESSAGE_COLUMS qual * vals
MULTIPLE_MESSAGE_COLUMS=1
#batch size for scan operations
COLUMN_RANGE_FILTER_BATCH_SIZE=4
EXCHG_PREFIX=EXCHG
TOPIC_PREFIX=TOPIC
UNIV_KEY_CTR=0

#==================
#HBase Connection
#==================
MAX_HTABLE_POOL_SIZE=4

#===========
#Test
#===========
#timeout for jdbc queries
PS_QUERY_TIMEOUT=100
#Run test at max throughput, no latching, short duration high load tests
MAX_TPUT=true
#Repeat the test infinitely using the same data file
REPEAT_DATA=false
READ_DATA=true
READ_CACHE_BLOCK_MULTIPLIER=1
READ_ONLY=false
SLEEP_AFTER_WRITE_BEFORE_READ_MS=60000
DELETE_EXISTING_TABLE=true

#RowKeyFilter or QualifierFilter based test for HBase fetch
ROWKEY_OR_COLQUAL_TEST=RowKeyFilter

#BATCH_MULTIPLE_BULK is number of multiples of  INSERT_BULK to read from the data file
#from this each INSERT_BULK is scheduled on the threadpool in chunks of THREAD_COUNT
BATCH_MULTIPLE_BULK=10
#INSERT_BULK shd be less than and multiple of BULK_SIZE
INSERT_BULK=300
FETCH_BULK=300
#Use batching. If false it iterates the array list and inserts one by one
INSERT_BATCH=true
# FETCH_BATCH works only with ROWKEY_OR_COLQUAL_TEST=BulkRowKey
# if FETCH_BATCH=false, we invoke rowkey scanner instead of batch gets
FETCH_BATCH=true
# will iterate over all the scanner records but not print them 
SCAN_ALL_RECORDS=true
ENABLE_SCAN_CACHE=true

# HBase operations rate
#NUM_MSGPUT_PER_SEC=4
#NUM_MSGREAD_PER_SEC=4

# HBase rate modeling (Time in seconds)
NUM_MSGPUT_MODEL=400#800#1200#1600
MSGPUT_TIME_MODEL=1#2#2#10
NUM_MSGREAD_MODEL=400#800#400#800
MSGREAD_TIME_MODEL=1#20#1#2


#HBase operations fetch/insert pool size ideally = NUM_MSGPUT_PER_SEC + NUM_MSGREAD_PER_SEC
THREAD_COUNT=16

#=============
#DBValidator
#=============
HTABLE_BATCH_SIZE=100000
# all will generate stats for all the rowkeys in the table
#HBASE_ROWKEY_LIST=all
# else specify a list of comma separated rowkey prefix
#HBASE_ROWKEY_LIST=Test-Exchange-588_Persistent-Test-Topic-588,Test-Exchange-842_Persistent-Test-Topic-842,Test-Exchange-764_Persistent-Test-Topic-764
# else specify range and start and end rowkey prefix along with start and end timestamp 
HBASE_ROWKEY_LIST=range
START_ROWKEY=Test-Exchange-100_Persistent-Test-Topic-100
END_ROWKEY=Test-Exchange-900_Persistent-Test-Topic-900
START_ROWKEY_TIMESTAMP=0
END_ROWKEY_TIMESTAMP=0
